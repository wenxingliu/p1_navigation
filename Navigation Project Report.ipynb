{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation Project Report\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The algorithm used in this project is Deep Reinforcement Learning with Double Q-learning and Experience replay.\n",
    "The solution proposed in this repo is able to solve the problem in less than 600 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Traning Scores](images/train_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Details\n",
    "\n",
    "There are three main components to this algorithm: \n",
    "\n",
    "- Environment\n",
    "- Agent\n",
    "- Q-Networks\n",
    "\n",
    "\n",
    "The environment is given. Therefore this algorithm focues on building Q Networks, the Agent, and how to train the agent/Q-Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm starts with initializing the Local Q Network and Target Q Network of the agent. The solution adopts a shallow Neural Network with only one hidden layer. Double Q-learning algorithm has two Q Networks. We call the first Q Network *Local Q Network*, which is used for selecting actions; the other is called *Target Q Network*, and it is used to evaluate actions. The two Q Networks have exact same structure, with only one fully-connected hidden layer that outputs 12 channels. The difference lies in how we update the two Q-Networks.\n",
    "\n",
    "The Local Q Network is updated every 4 steps, and the Target Q Network is updated incrementally by the difference between two Q Networks every 4 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During each episode, the enviroment randomly selects a starting point, i.e. an initial state. The agent uses epsilon-greedy method to select an action. And the environment reacts to the chosen action and the agent receives an reward and ends in the next state. The value of epsilon in Epsilon-greedy algorithm starts at 1.0 (uniformly random), and decays by 0.99 at every episode. At the end of each step, the agent saves this experience to its memroy.\n",
    "\n",
    "The algorithm does not update Q Networks at every step, but instead it updates them at every 4 steps. When it is time to update the Q-Networks, the agent randomly (uniformly) samples a batch of 64 steps from its memory, and calculates the expected state-action value of each sampled experience based on Local Q-Network, and compares them to their state value based on Target Q-Network. We uses MSE function to compute the loss, and the agent runs backprop on the loss function, and finally updates the Local Q-Network using a Adam optimizer. Learning rate is set to 0.001 at the beginning, and decays by half at every 100 episodes. The reasoning behind this is, we would like to reduce the step size when we are getting closer and closer to the optimal point, so that we do not step over it and head towards a wrong direction.\n",
    "\n",
    "The Target Q-Network is also updated and \"synced\" with the Local Q-Network everytime the agent learns, which is every 4 steps. The Target Q-Network is updated incrementally by the difference between two Q Networks every 4 steps, as shown in the following equation, and $\\tau = 0.001 $. \n",
    "\n",
    "$\\theta_{target} = (1 - \\tau) \\cdot \\theta_{target} + \\tau \\cdot (\\theta_{target} - \\theta_{local})$\n",
    "\n",
    "where $\\tau = 0.001$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent is evaluated based on the average score of the last 100 consecutive episodes. We trained the agent for 1000 episodes, and the agent steadily performs an average score of over 13 in less than 600 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of hyper-parameters used in this solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "FC_SIZE = 12\n",
    "BUFFER_SIZE = int(1e5)\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-3\n",
    "LR = 0.001\n",
    "UPDATE_EVERY = 4\n",
    "EPS_DECAY = 0.99\n",
    "EPS_MIN = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the trained agent as described in previous section to perform 100 episodes to evaluate the results we see during training. The result is shown below. The grey dashed line is our target average score, which is 13.0. The red dashed line is the average score from the validation test, which is 13.8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Validation Scores](images/validation_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a few things that can be explored in future work:\n",
    "\n",
    "- A deeper Neural Nets structure for Q-Networks.\n",
    "- Different learning rate, and learning rate decay combinations.\n",
    "- Hyperparatmeter $\\tau$ in Target Q-Network soft update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
